{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5f15f90-68bc-4d41-add1-aeaee8a9d21a",
   "metadata": {},
   "source": [
    "# Lecture 2: Data Preparation\n",
    "\n",
    "In this lesson you'll carry out some of the data cleaning steps required to prepare data for pretraining. In the video, Sung mentioned an Upstage tool called **Dataverse** which can help you with data cleaning. You can checkout the features of Dataverse at [this link](https://github.com/UpstageAI/dataverse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f72caad8-34d6-4deb-a7f4-49d8d67925cd",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4772e5df",
   "metadata": {},
   "source": [
    "## 1. Sourcing datasets for pretraining\n",
    "\n",
    "In this section, you'll see two ways to source data for training:\n",
    "1. Download an existing dataset from Hugging Face\n",
    "2. Create a dataset of python scripts sourced from Github\n",
    "\n",
    "In both cases the result will be a Hugging Face `Dataset` object, part of the `Datasets` library. You can read more about the properties of Datasets and how to work with them on the [Hugging Face website](https://huggingface.co/docs/datasets/en/index).\n",
    "\n",
    "### Download data from Hugging face\n",
    "\n",
    "The dataset you download here is a subset of a much larger dataset called **Red Pajama**. The full, 1 trillion token dataset is available on Hugging Face at [this link](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f287ed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kichinosukey-mba/pretrainingLLMs/.venv/bin/python\n",
      "exe: /Users/kichinosukey-mba/pretrainingLLMs/.venv/bin/python\n",
      "find_spec: ModuleSpec(name='datasets', loader=<_frozen_importlib_external.SourceFileLoader object at 0x1064d95d0>, origin='/Users/kichinosukey-mba/pretrainingLLMs/.venv/lib/python3.11/site-packages/datasets/__init__.py', submodule_search_locations=['/Users/kichinosukey-mba/pretrainingLLMs/.venv/lib/python3.11/site-packages/datasets'])\n"
     ]
    }
   ],
   "source": [
    "import sys;\n",
    "print(sys.executable)\n",
    "\n",
    "import sys, importlib.util, subprocess, json\n",
    "print(\"exe:\", sys.executable)\n",
    "print(\"find_spec:\", importlib.util.find_spec(\"datasets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f7bc396b-faaa-452a-be0e-7b17a79b8e0e",
   "metadata": {
    "height": 197
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "pretraining_dataset = datasets.load_dataset(\n",
    "    \"togethercomputer/RedPajama-Data-V2\",\n",
    "    name=\"default\",\n",
    "    partition=\"head_middle\",\n",
    "    snapshots=[\"2023-06\"],\n",
    "    languages=[\"en\"],\n",
    "    streaming=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae796e0e-c719-4b39-be29-286aa5a436fb",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDatasetDict({\n",
      "    train: IterableDataset({\n",
      "        features: ['raw_content', 'doc_id', 'meta', 'quality_signals'],\n",
      "        n_shards: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(pretraining_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4266d",
   "metadata": {},
   "source": [
    "Only work with the `text` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3765e9c6-5f18-4325-8157-206e3bb199fa",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "pretraining_dataset = pretraining_dataset.select_columns(\n",
    "    [\"raw_content\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84aaf8af-c20b-49ea-ba75-65ea7f87c146",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDatasetDict({\n",
      "    train: IterableDataset({\n",
      "        features: ['raw_content'],\n",
      "        n_shards: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(pretraining_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "366f280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_content を text にリネーム\n",
    "pretraining_dataset = pretraining_dataset.rename_column(\"raw_content\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "16bff0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 先に欲しい件数だけ集める\n",
    "max_samples = 10_000\n",
    "rows = []\n",
    "for i, row in enumerate(pretraining_dataset[\"train\"]):\n",
    "    rows.append(row)  # row は dict 形式\n",
    "    if i + 1 >= max_samples:\n",
    "        break\n",
    "\n",
    "# Iterable → Dataset へ変換\n",
    "pretraining_dataset = datasets.Dataset.from_list(rows)  # or Dataset.from_dict({k: [r[k] for r in rows] for k in rows[0]})\n",
    "print(pretraining_dataset)  # -> Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ca9d93",
   "metadata": {},
   "source": [
    "### Compare pretraining and fine-tuning datasets\n",
    "In the next cell, you'll download a fine-tuning dataset to contrast with the pretraining dataset you loaded above. You can read more about the Alpaca model and instruction tuning dataset [here](https://crfm.stanford.edu/2023/03/13/alpaca.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5fb48e62-11fa-47a3-8e2c-8b9ae4011c7f",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 52002\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "instruction_dataset = datasets.load_dataset(\n",
    "    \"c-s-ale/alpaca-gpt4-data\",\n",
    "    split='train'\n",
    ")\n",
    "print(instruction_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "768b3d25-2bca-4a93-b3b0-bd05c55bf70e",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Give three tips for staying healthy.\n",
      "Input: \n",
      "Output: 1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
      "\n",
      "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(\"Instruction: \" + instruction_dataset[i][\"instruction\"] \n",
    "      + \"\\nInput: \" + instruction_dataset[i][\"input\"] \n",
    "      + \"\\nOutput: \" + instruction_dataset[i][\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef4d82",
   "metadata": {},
   "source": [
    "Notice how in contrast to the pretraining data, which is just raw text, fine-tuning datasets are structured into question-answer pairs or instruction-response sets that can include additional input context if required. \n",
    "\n",
    "Moving forward, you'll only work with the unstructured pretraining dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6288d0f",
   "metadata": {},
   "source": [
    "### Scrape python code from Github\n",
    "Here, you'll download a selection of python scripts from Github and then prepare them as a Hugging Face `Dataset` object to use in training. \n",
    "\n",
    "The same pattern here will work for preparing any text scraped from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c14d65d1-1a52-4777-95f8-4a2f84a3c9ac",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "# Import some required packages\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Path to directory to store python scripts\n",
    "code_dir = \"./code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7e4a0b0b-2e76-4567-9ef5-f3b6d8020851",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/TheAlgorithms/Python/master/searches/double_linear_search_recursion.py\",\n",
    "    \"https://raw.githubusercontent.com/KosingZhu/tensorflow/master/tensorflow/python/tools/module_util.py\",\n",
    "    \"https://raw.githubusercontent.com/EricRemmerswaal/tensorflow/master/tensorflow/python/distribute/distribute_coordinator_context.py\",\n",
    "    \"https://raw.githubusercontent.com/computationalartist/tensorflow/master/tensorflow/python/ops/numpy_ops/integration_test/benchmarks/numpy_mlp.py\",\n",
    "    \"https://raw.githubusercontent.com/Van-an/tensorflow/master/tensorflow/python/distribute/coordinator/values.py\",\n",
    "    \"https://raw.githubusercontent.com/nkgwer/tensorflow/master/tensorflow/lite/tools/visualize.py\",\n",
    "    \"https://raw.githubusercontent.com/gitblazer/youtube-dl/master/youtube_dl/version.py\",\n",
    "    \"https://raw.githubusercontent.com/Joshua-Barawa/My-Photos/master/venv/lib/python3.8/site-packages/django/contrib/messages/__init__.py\",\n",
    "    \"https://raw.githubusercontent.com/PaliC/pytorch/master/test/fx/test_subgraph_rewriter.py\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1818ac26",
   "metadata": {},
   "source": [
    "Retrieve the python scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4bc904ed-da23-40c0-b600-bcb1feebbd31",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on url: https://raw.githubusercontent.com/TheAlgorithms/Python/master/searches/double_linear_search_recursion.py\n",
      "Working on url: https://raw.githubusercontent.com/KosingZhu/tensorflow/master/tensorflow/python/tools/module_util.py\n",
      "Working on url: https://raw.githubusercontent.com/EricRemmerswaal/tensorflow/master/tensorflow/python/distribute/distribute_coordinator_context.py\n",
      "Working on url: https://raw.githubusercontent.com/computationalartist/tensorflow/master/tensorflow/python/ops/numpy_ops/integration_test/benchmarks/numpy_mlp.py\n",
      "Working on url: https://raw.githubusercontent.com/Van-an/tensorflow/master/tensorflow/python/distribute/coordinator/values.py\n",
      "Working on url: https://raw.githubusercontent.com/nkgwer/tensorflow/master/tensorflow/lite/tools/visualize.py\n",
      "Working on url: https://raw.githubusercontent.com/gitblazer/youtube-dl/master/youtube_dl/version.py\n",
      "Working on url: https://raw.githubusercontent.com/Joshua-Barawa/My-Photos/master/venv/lib/python3.8/site-packages/django/contrib/messages/__init__.py\n",
      "Working on url: https://raw.githubusercontent.com/PaliC/pytorch/master/test/fx/test_subgraph_rewriter.py\n"
     ]
    }
   ],
   "source": [
    "for url in urls:\n",
    "    print(f\"Working on url: {url}\")\n",
    "    response = requests.get(url)\n",
    "    file_name = os.path.basename(url)\n",
    "    file_path = os.path.join(code_dir, file_name)\n",
    "    \n",
    "    with open(file_path, \"wb\") as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "807f9164-ed85-4df8-bb01-e5ed5a41cba5",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_subgraph_rewriter.py\n",
      "numpy_mlp.py\n",
      "values.py\n",
      "version.py\n",
      "double_linear_search_recursion.py\n",
      "__init__.py\n",
      "visualize.py\n",
      "module_util.py\n",
      "distribute_coordinator_context.py\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(code_dir)\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7561587",
   "metadata": {},
   "source": [
    "Concatenate scripts into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0c9b603c-9fd4-410b-b013-802a6d055c49",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "code_dataset = []\n",
    "for file in os.listdir(code_dir):\n",
    "    code_dataset.append(\n",
    "        {'text': open(os.path.join(code_dir, file), 'r').read()}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df9274",
   "metadata": {},
   "source": [
    "Convert list to Hugging Face `Dataset` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cfa9b259-5434-4d2c-96a1-d0c68149a3cd",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 9\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "code_dataset = datasets.Dataset.from_list(code_dataset)\n",
    "print(code_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33dce90",
   "metadata": {},
   "source": [
    "Combine the python code dataset with the pretraining dataset you downloaded above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "192b52a7-edfa-4564-ad8c-596cc2537831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 9\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(pretraining_dataset)\n",
    "print(code_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3d2b129e-4307-4b8c-a94c-b348fd66a7fe",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['AMICUS ANTHOLOGIES, PART ONE (1965-1972)\\nFebruary 23, 2017 Alfred Eaker Leave a comment\\nWith Dr. Terror’s House of Horrors (1965, directed by Freddie Francis and written by Milton Subotsky) Amicus Productions (spearheaded by Subotsky and Max Rosenberg, who previously produced for Hammer and was a cousin to Doris Wishman) established itself as a vital competitor to Hammer Studios. Rather than imitating Hammer’s modernization of Gothic classics, Amicus developed its own niche with omnibus films. They were successful enough to be in full-fledged production for a decade, establishing a reputation as the go-to studio for horror anthologies. This, their introductory portmanteau film, clearly influenced by EC Comics, sets a pattern of to-be-expected unevenness. Still, Amicus installs themselves as a horror studio to be reckoned with, sparing no expense in procuring Hammer’s top actors: Peter Cushing (who would star in all but one of the Amicus anthologies) and Christopher Lee. For its wraparound segment, Dr. Terror’s House of Horrors opens to the duo (among other passengers) on a train. Dr. Schreck ((“Shreck” is German for “terror,” and a nod to the famous star of F.W. Murnau‘s Nosferatu.)) (Cushing, saddled with a terrible German accent and glued on brows) pulls out a deck of tarot cards. “Pick a card, any card, and tape it three times,” Schreck tells his fellow passengers. Each participant will hear of a fate that may await them. Among the passengers is Christopher Lee who will, of course, factor into one of the five narratives.\\nIn “Werewolf,” Neil McCallum is an architect renovating an old dark house, which turns out to be cursed. The title monster is featured in this pedestrian tale of ancestral revenge with a “twist.”\\nWith Alan Freeman (better known as the U.K D.J. for “Pick of the Pops”) served up as a snack for a venus fly trap, “The Creeping Vine,” thankfully doesn’t take itself so seriously. It is refreshingly lightheaded hokum.\\n“Voodoo” is the worst of the lot; badly dated in its stereotypes, with Kenny Lynch belting out a stolen voodoo tune.\\n“Disembodied Hand,” has elitist art critic Franklin Marsh (Lee) driving artist Eric Landor (Michael Gough) to suicide. Landor’s severed hand returns to exact revenge on the mean critic. It’s in the spirit of The Beast with Five Fingers, among others, and chock-full of two-dimensional caricatures of both artists and critics. It holds no surprises, but with Lee and Gough engaged in a bit of whistling-while-they-work fun, it’s easily the best episode.\\n“Vampire” feature a young Donald Sutherland who discovers he is married to… a vampire! It barely raises a pulse.\\nSeen today, Dr. Terror’s House of Horrors is more camp than horror, and its appeal is one of genre nostalgia. Still, the phenomenal box office success of Dr. Terror green-lighted a second portmanteau film in 1967, entitled Torture Garden (directed by Freddie Francis and written by Robert Bloch). It contains no torture nor any garden. Burgess Meredith (in a preposterous disguise, reminding us of the Penguin) is Old Nick himself, going by the pseudonym of Dr. Diablo and moonlighting as a carnival barker who promises a tortuous exhibit that can reveal the future. “You’ll shake, you’ll shiver, but it’s all good fun,” Diablo hammily tells his patrons. Unfortunately, only one of the four tales lives up to that promise.\\n“Enoch,” is the opening narrative. Michael Bryant’s inheritance money (from an uncle who took his time dying) is going to be spoiled by a mean ol’ puddy tat with a lot of doubloons.\\n“Over Hollywood” has Beverly Adams discovering the fountain of youth in Hollywood with robotic consequences.\\n“Mr. Steinway” might be seen as a poor precursor to Stephen King’s “Christine,” replacing a killer car with a killer piano. It’s as absurd as it sounds.\\nThe first three segments are sloppily written and executed with little enthusiasm; each progressively worse, but the final segment single-handedly salvages the anthology.\\n“The Man Who Collected Poe” finds Jack Palance (playing against type) as an Edgar Allan Poe-obsessed geek who may have found his soulmate in fellow fanatic Peter Cushing. However, somebody’s got something—or someone—hidden in the basement and … somebody’s got the fever, which leads to a fiery finale. Cushing and Palance clearly enjoyed playing opposite one another and their chemistry, along with clever writing, making one wish the previous segments had been as enjoyable.\\n1970’s The House That Dripped Blood (directed by Peter Duffell and written by Robert Bloch) is a considerable improvement over its predecessors. Duffell lacks the visual astuteness of Freddie Francis, but he has superior stories to work with and a top notch cast. The connecting theme is the titular house, which has a bit of baggage left over from all who have resided there.\\nIn “Method For Murder,” Denholm Elliott is a horror author who writes a character that becomes a tad too three-dimensional, much to his wife’s peril.\\n“Waxworks” stars Cushing as an uptight retired stockbroker and lifelong bachelor who visits a wax museum, only to see a figure of a woman whom he once was in love with. Obsession and unrequited love naturally go hand-in-hand, or head-on-plate.\\nIn “Sweets to the Sweet,” Nyree Dawn Porter is hired to tutor a young, motherless child (Chloe Franks) who is unloved by her cold-hearted father, Christopher Lee. Without giving too much away, let’s just say the underlying theme is one few filmmakers would dare tackle today.\\n“The Cloak” is the most famous of the four episodes, remembered fondly for its absurd humor. It stars John Pertwee (best known for his portrayal of Dr. Who) as an actor who mantles the cloak of a purported actual vampire. Hammer favorite Ingrid Pitt bares her fangs and, of course, a bit more.\\nAll four episodes feature strong acting, which is a rarity in contemporary horror and should be a model for genre filmmakers. Elliot’s restrained performance in “Method For Murder” is admirable enough to forgive the predictable “twist.” The stylish “Waxworks” features an equally stylish performance from Cushing, although narratively it is the thinnest episode. “Sweets to the Sweet” is psychologically intense with three powerhouse performances, making it the strongest entry. Although John Pertwee is a bit on-the-sleeve in “The Cloak,” his performance suits the tone; but, he’s no match for Pitt.\\nAsylum (1972, directed by Roy Ward Baker and written by Robert Bloch) is often cited as the best of the Amicus anthologies. It opens on Dr. Martin (Robert Powell, best known as the blue-eyed Anglo-Saxon savior plopped into the Middle East in Franco Zeferelli’s Jesus of Nazareth) showing up for his scheduled job interview with Dr. Starr for a position at the Dunsmoor Asylum. Martin is met by Dr. Rutherford (Patrick Magee), however, and informed that Dr. Starr is now a patient after going insane and becoming violent. Rutherford devises a test for Martin: he will interview four patients and if he can guess which one is Dr. Starr, then he will be hired. Naturally, this segues into four tales from Mr. Bloch.\\nIn “Frozen Fear,” Walter (Richard Todd) is having a sordid affair with Bonnie (Barbara Parkins). When his wife Ruth (Sylvia Syms) won’t give him a divorce, Walter grabs an axe and fills his basement freezer with prime ex-wife cold cuts. Little does Walter know that the wifey was a voodoo priestess, and when that freezer thaws, big things come a-crawling in small packages—lots of them. This vignette is the most blatantly indebted to EC comics and, as such, it’s probably Amicus’ finest twenty minutes.\\n“The Weird Tailor” opens with tailor Bruno (Barry Morse) on the verge of being evicted. As luck would have it, Mr. Smith (Peter Cushing) walks into Bruno’s shop and orders a unique suit. With the promise of a hefty commission, Bruno obsessively begins working according to Mr. Smith’s very specific instructions. Unknown to Bruno, the suit is meant to resurrect Mr. Smith’s recently deceased son. Things don’t go according to plan. Previously adapted for Boris Karloff’s “Thriller”, this one can’t match the TV effort. Given a shorter running time for Asylum, Bloch was forced to excise the prologue, and with it gone, the suspense and menace are diminished. The original thriller was actually more perverse in suggesting Bruno’s wife’s sexual attraction to a mannequin. Additionally, Bruno’s character was less sympathetic, bringing a pronounced, and weird, abusive quality that is merely sketched here. Cushing is superb, bringing a sense of pathos to the character, but his part is little more than a cameo. Being more compressed, the schlock quality of the ending is more pronounced. Yet, for all of its comparative flaws, this is a strong second episode.\\n“Lucy Comes To Stay” is the weakest of the four episodes. Barbara (Charlotte Rampling) has been released from the asylum to the care of her brother, George (James Valliers) and nurse Higgins (Megs Jenkins). However, Barbara has an imaginary friend named Lucy (Britt Ekland) who doesn’t care for George or the nurse. Lucy is also handy with a knife. Disappointingly, it plays out exactly as expected, and isn’t helped by lackluster performances (Rampling being the exception).\\nSurprisingly, “Mannequins of Horror” is an extension of the wraparound, with the arrival of a new doctor named Byron (Herbert Lom) who has the demonic hobby of placing spirits within miniature robots and imbuing them with life, which serves as a potential gateway to immortality. Dr. Martin returns to uncover Dr. Starr’s identity in a delightfully unpleasant ending. It’s something of a mini-masterpiece that clearly proved an inspiration to later, albeit inferior films.\\nPART TWO will begin with Tales From The Crypt (1972) and take us to the final Amicus anthology: From Beyond The Grave (1974).\\nAnthologyChristopher LeeFreddie FrancisHorrorPeter CushingPeter DuffellRoy Ward Baker\\nPrevious Post271. THE HOURGLASS SANATORIUM (1973)Next PostWEIRD HORIZON FOR THE WEEK OF 2/24/2017', 'Banc Texas Allen Parkway, completed 1983\\nBanc Texas Allen Parkway, completed 1983, Box: 6, Folder: 4. Arthur E. Jones Architectural Records, MS 535. Woodson Research Center, Rice University, Houston, Texas. http://archives.library.rice.edu/repositories/2/archival_objects/79290 Accessed January 26, 2023.', 'Ohmydollz Frogitaire Jurassic Pinball Kore Putt Sling Jumper 2\\nFranckfort\\nCalifornia To Enforce Stringent Auto Emission Regulations\\nEarlier, the state of California issued proposed emissions regulations that are expected to be enforced in the next decade. The proposed regulations, when approved, could force automakers to reduce tailpipe emissions of greenhouse gases. The latter are intimately related to global warming. Parenthetically, the state is aiming to reduce 30 percent of harmful gas emissions by passing stringent auto regulations.\\nCalifornia represents about 10 per cent of the overall American auto market. Auto analysts expect that if the proposed regulations are adopted later this year after the review process, it would be the very first limits in the United States regarding auto emissions of gasses linked to global warming. In addition, other states in the nation are expected to follow same regulations to further promote the wellbeing of its citizens and their environment. At least four Northeastern states are observing what California is currently doing.\\nThe California Air Resources Board, the one responsible of the proposed regulations, said that the stringent emission regulation is made in response to a state law enacted in 2002. Said law requires automakers to produce cars, pickup trucks, sport utility vehicles and minivans that emit 30 per cent less greenhouse gases on average by 2015.\\nThe proposed regulations are expected to increase the market value of new vehicles. However, the boards did not go into specifics by divulging the anticipated increase. With the proposed regulations, automotive emission parts are expected to become expensive due to the upgrades and innovations to come up with reduced greenhouse gases emissions. On the one hand, automakers are trying to prevent the proposal from becoming a law. California\\'s actions have tremendous repercussions for the auto industry because the state represents about 10 per cent of the nation\\'s car market.\\nThe proposal entails added expenditures to automakers because of the required studies, testing, materials and modifications. Consequently, there is no assurance of a greener pasture attributed to the enhancements of emission control parts.\\nThe upgrades needed to meet California\\'s proposed regulations could add thousands of dollars to the price of a new vehicle, said Eron Shosteck of the Alliance of Automobile Manufacturers. \"We are happy to offer these efficient fuel-saving technologies. But consumers just don\\'t want to buy them... Automakers believe consumers should have the option to choose.\" He added that the regulations would result in vehicles that are \"lighter, smaller, less powerful, and less able to do what consumers want.\"\\nCalifornia has the dirtiest air in America. This is the very reason why several movements have been established to take care of the matter. In addition to these cause-oriented movements, the state\\'s Air Resources Board is also determined to promote a cleaner air to preclude illness and other hazardous effects of global warming.\\nGlobal warming, the continuous increase of Earth\\'s temperature, is said to be mostly attributable to human activities that results to increased atmospheric concentration of greenhouse gases like carbon dioxide, a colorless gas that rises into the atmosphere and traps heat. The situation further leads to warming of the atmosphere by escalating the greenhouse effect.\\nGlobal warming influences both the natural environment and human life. Its effects include sea level rise, agricultural repercussions, thinning of the ozone layer, extreme weather events and spread of disease like malaria and other infectious maladies.\\nCalifornia officials said automakers could reduce carbon dioxide emissions by improving gas mileage. Less fuel consumption would result in fewer emissions. They further noted that gas mileage could be improved with available technology, such as high-tech transmissions that shift automatically into the most efficient gear.\\nAbout the Author: Joe Thompson is the owner of a successful auto body shop in Ferndale, California. This 38 year old is also a prolific writer, contributing automotive related articles to various publications.', 'What our clients say about our service.\\nThanks for the great Job! (L.C.)\\nTI loved working with Bud and Norman. They were so thorough and explained everything so well, both verbally and in the written report. I was able to look back at the detailed report and refer to pictures and descriptions when speaking to my contractors, which was a great help.\\nMr. Albertson’s reports are nicely detailed, and his explanations well-grounded in reality. He made it easy for us to triage what needed to be done right away, vs things that we can wait to address.\\nWe have used Bud twice now both for an existing older home and a new construction. In each circumstance he is very thorough and thoughtful in his reporting.\\nThank you for your services Bud and Norm. They were very helpful in uncovering additional factors with the home that I had not anticipated. (J.W.)\\nI enjoyed meeting you both yesterday and thank you again for shining in your profession. You were very informative and your report confirms it. Thanks for aiding us in being more informed concerning our major purchase. Merry Christmas to you both! (W.J.)\\nThank you Bud! It is a pleasure working with you! (S.B.)\\nThanks for making this an enjoyable experience for us! I thought y’all were very professional ! (K.R.)\\nI can say that I had a pleasant experience with using this company for my home inspections. My report was easy to read and I have actually been using it to help me keep track of what needs to be done. I would highly recommend this company to anyone that wants a detailed Home Inspection (N.S.)\\nVery thorough and impactful toward making final negotiations on the price. The report was easy to read. The face-to-face explanation was friendly and professional. Thank you. (R.O.)\\nI felt your inspection was thorough and easy to understand. Our seller accepted your report and did a good job of making the necessary repairs. Thank you. (R.N.)\\nExcellent all the way around! Thanks! (J.C.)\\nSuperior service all around. Extremely thorough process and very comfortable people to work with (A.P.)\\nAs investors, we have worked with several different inspectors. This is the first time we have felt completely satisfied, and did not feel that we needed to keep searching for an inspector who would give us better service. Glad we found you! (J.L.)\\nCould not be more pleased with your service, thank you so much Bud. (W.H.)\\nPleasant experience. Very informative. (S.D.)\\nI was very pleased with the amount of information given and the explanation of it all, along with the pictures. I have been amazed with the follow up contact. Thank you so greatly and may God continue to bless all of your endeavors, personal and business wise. (T.R.)\\nThank you so much for the reports. Also thank you again for spending the time with me to explain each item and using the pictures as examples. Your report will help to identify what to do and where. It will certainly make it easy for me gets these items corrected in a timely manner.\\nAfter working with you, I can not understand why all inspectors do not do their job the way you do yours. It was a pleasure.\\nChristine (C.)\\nI just wanted to say thank you for doing such a great job on the inspection, your thoroughness means alot. I am not purchasing this home but, will definitely use you as the inspector on the next house I find. (S.F.)\\nService was greatly appreciated, thank you so much (P.O.)\\nthanks for everything. The service was great (S.O.)', \"Updaty posty thingy\\nSooo....\\nChainmaille was a disaster. I need someone to show me how to construct. It was moot anyway, as I had an anxiety attack and barely made it in to the con. I am so embarased, but glad it wasn't a long term thing.\\nI am still working on getting the chaim maille done. Maybe it will look fine. I don't know. I also need to work on the scale spoon maille.\\nRight now, though, my main focus is finding a job. I thought I had extended unemployment until I was done wtih school. Turns out that was not entirely true, and now I am kind of up a creek. I have been saving money, so right now I have been paying bills with my savings. However that is also about to run out. I have applied for abawd. I do not know if I will get it, or how long it will take to kick in. Hopefully soon. I have had a bunch of interviews, and I am waiting to hear back. I have shotgunned a ton of applications for any and all jobs, no matter what they pay or how much experience they ask for. In the mean time I am kind of stuck. If you are reading this and feel like helping out, feel free to click the Tips/Supply Fund button and give a little. It would be much appreciated. Also, if you have job leads in the Portland, OR area, feel free to leave them in the comments. I am kind of tired of being in this spot. It seems I am here every couple of years. I just want to find a job where I can stay for 5-10 years without worrying about where the next paycheck is going to come from. Apparently that is too much to ask from the universe. Sigh.\"]}\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.concatenate_datasets(\n",
    "    [pretraining_dataset, code_dataset]\n",
    ")\n",
    "print(dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf07e4-5be3-4e1f-99e0-c14013d10fd3",
   "metadata": {},
   "source": [
    "## 2. Data cleaning\n",
    "\n",
    "In the cells below, you'll carry out the following cleaning steps:\n",
    "1. Filter out samples that are too short\n",
    "2. Remove repetitions within a single text example\n",
    "3. Remove duplicated documents\n",
    "4. Quality filter to remove non-English texts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c657a5ba",
   "metadata": {},
   "source": [
    "### Remove examples that are too short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a75dcfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10009"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b6f293b4-486d-4361-ba9f-6038864e67de",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def paragraph_length_filter(x):\n",
    "    \"\"\"Returns False iff a page has too few lines or lines are too short.\"\"\"\n",
    "    lines = x['text'].split('\\n')\n",
    "    if (\n",
    "        len(lines) < 3\n",
    "        or min(heapq.nlargest(3, [len(line) for line in lines])) < 3\n",
    "    ):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2755ea1a-a9d3-4771-950e-73304d21252b",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e456f8a69a14ba988eaac723c4f93b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.filter(\n",
    "    paragraph_length_filter,\n",
    "    load_from_cache_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a87c47c4-9331-4b44-a5e2-6b81dfbd7bf1",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8966"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced32d0",
   "metadata": {},
   "source": [
    "### Remove repeated text within training examples\n",
    "\n",
    "Here you'll remove text repetitions within each example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6b0a79c4-e714-42d4-bdb7-d3930d74c5de",
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "def find_duplicates(paragraphs):\n",
    "    \"\"\"\n",
    "    Use this function to find the number of repetitions \n",
    "    in the paragraphs.\n",
    "    \"\"\"\n",
    "    unique_x = set()\n",
    "    duplicate_chars = 0\n",
    "    duplicate_elements = 0\n",
    "    for element in paragraphs:\n",
    "        if element in unique_x:\n",
    "            duplicate_chars += len(element)\n",
    "            duplicate_elements += 1\n",
    "        else:\n",
    "            unique_x.add(element)\n",
    "    return duplicate_elements, duplicate_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f5dc415c-35e1-4d63-95bc-74775bfdaba4",
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def paragraph_repetition_filter(x):\n",
    "    \"\"\"\n",
    "    Returns False iff a page has too many repetitions.\n",
    "    \"\"\"\n",
    "    text = x['text']\n",
    "    paragraphs = re.compile(r\"\\n{2,}\").split(text.strip())                # Split by paragraphs (2 or more newlines)\n",
    "    paragraphs_duplicates, char_duplicates = find_duplicates(paragraphs)  # Find number of duplicates in paragraphs\n",
    "    if paragraphs_duplicates / len(paragraphs) > 0.3:\n",
    "        return False\n",
    "    if char_duplicates / len(text) > 0.2:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "522708f3-58fa-4f7e-bf34-a3f282b309e4",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7fc3edb09c4b8cb6d1c712e4baeedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/8966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.filter(\n",
    "    paragraph_repetition_filter,\n",
    "    load_from_cache_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e81f0202-b3ef-4953-8203-5062ad20402d",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8965"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b4f79",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "\n",
    "In this section, you'll remove duplicate examples from the entire dataset (in contrast to the previous step where you were just looking for repeated text in each example.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b8dcd316-20f1-4def-959e-ce3dfe9426c4",
   "metadata": {
    "height": 268
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcd519f668843d9aca7209cb70149f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/8965 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def deduplication(ds):\n",
    "    def dedup_func(x):\n",
    "        \"\"\"Use this function to remove duplicate entries\"\"\"\n",
    "        if x['text'] in unique_text:\n",
    "            return False\n",
    "        else:\n",
    "            unique_text.add(x['text'])\n",
    "            return True\n",
    "\n",
    "    unique_text = set()\n",
    "\n",
    "    ds = ds.filter(dedup_func, load_from_cache_file=False, num_proc=1)\n",
    "    return ds\n",
    "\n",
    "dataset = deduplication(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "754444d7-def6-499c-9da8-d18136a4ee6b",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8965"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eb3a2c",
   "metadata": {},
   "source": [
    "### Quality filter - Language\n",
    "\n",
    "Here you'll remove any text examples that are in a language other than English. The code here uses a language detection model called fastText. You can read about fastText [here](https://fasttext.cc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "64ae5c3e-ae16-495c-a651-0b0200169569",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# !pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f11bc056-faeb-470d-acc2-9c1bae47c3c1",
   "metadata": {
    "height": 319
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function english_language_filter.<locals>.is_english at 0x3221fe480> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426dd043190749d1a6cdeb10d0bc1eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/8965 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import urllib\n",
    "from fasttext.FastText import _FastText\n",
    "\n",
    "def english_language_filter(ds):\n",
    "    # load language detection model\n",
    "    model = _FastText('./models/upstage/L2_language_model.bin')\n",
    "    \n",
    "    def is_english(x):\n",
    "        # Predict language of the text and probability\n",
    "        language, score = model.predict(x['text'].replace(\"\\n\", \"\"))\n",
    "\n",
    "        language = language[0].split(\"__\")[2]\n",
    "        return score > 0.4 and language == \"en\" # change code here if building a model in another language\n",
    "\n",
    "    ds = ds.filter(is_english, load_from_cache_file=False, num_proc=1)\n",
    "    return ds\n",
    "\n",
    "dataset = english_language_filter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f71f18f5-53b6-42fa-bb3a-d321904e3dc1",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8963"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428dcbc5",
   "metadata": {},
   "source": [
    "## 3. Save the dataset to disk\n",
    "\n",
    "Read more about the parquet data format [here](https://parquet.apache.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "21e68cc7-058a-4783-9dd5-96c2ef9a752e",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa7a397e1aa465993ff91945276ac82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "48710013"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./data/preprocessed_dataset.parquet\"\n",
    "dataset.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25c3b7-854c-46e2-8405-3cd3994bf089",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5933c23-07d1-4db9-a3c8-164fc9c5ae93",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2cb60-e263-4f0a-8c41-eea29a210c1e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrainingLLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
